{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8855a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents.middleware import (\n",
    "    AgentMiddleware,\n",
    "    AgentState,\n",
    "    ModelRequest,\n",
    "    ModelResponse,\n",
    "    before_agent,\n",
    "    after_agent,\n",
    "    before_model,\n",
    "    after_model,\n",
    "    wrap_model_call,\n",
    "    hook_config,\n",
    ")\n",
    "from langgraph.runtime import Runtime\n",
    "from typing_extensions import NotRequired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f130c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf7b5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('agent.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6cd7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_ENDPOINT = \"https://dhanush-ai507.cognitiveservices.azure.com/\"\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME= \"gpt-4.1\"\n",
    "AZURE_OPENAI_API_VERSION= \"2024-12-01-preview\"\n",
    "AZURE_OPENAI_API_KEY= \"CGSB7TAAbigunwOWa1mKRqVtn6q1q6adAPwySS7B1TuxLNXKlhtoJQQJ99BKACYeBjFXJ3w3AAAAACOGeW9\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4971f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AzureOpenAIConfig:\n",
    "    \"\"\"Production Azure OpenAI configuration with validation and health checks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize and validate Azure OpenAI configuration.\"\"\"\n",
    "        # Load configuration from environment\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        self.endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        self.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "        self.deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4.1\")\n",
    "        self.gpt4_deployment_name = os.getenv(\"AZURE_OPENAI_GPT4_DEPLOYMENT_NAME\", \"gpt-4.1\")\n",
    "        \n",
    "        # Configuration limits\n",
    "        self.max_retries = int(os.getenv(\"MAX_RETRIES\", \"3\"))\n",
    "        self.rate_limit_calls = int(os.getenv(\"RATE_LIMIT_CALLS_PER_MINUTE\", \"60\"))\n",
    "        self.max_conversation_messages = int(os.getenv(\"MAX_CONVERSATION_MESSAGES\", \"100\"))\n",
    "        self.request_timeout = int(os.getenv(\"REQUEST_TIMEOUT\", \"60\"))\n",
    "        \n",
    "        # Validate configuration\n",
    "        self._validate()\n",
    "        \n",
    "    def _validate(self):\n",
    "        \"\"\"Validate configuration and fail fast if invalid.\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        if not self.api_key:\n",
    "            errors.append(\"AZURE_OPENAI_API_KEY is required\")\n",
    "        \n",
    "        if not self.endpoint:\n",
    "            errors.append(\"AZURE_OPENAI_ENDPOINT is required\")\n",
    "        elif not self.endpoint.startswith(\"https://\"):\n",
    "            errors.append(\"AZURE_OPENAI_ENDPOINT must start with https://\")\n",
    "        \n",
    "        if not self.deployment_name:\n",
    "            errors.append(\"AZURE_OPENAI_DEPLOYMENT_NAME is required\")\n",
    "        \n",
    "        if errors:\n",
    "            error_msg = \"Configuration validation failed:\\n\" + \"\\n\".join(f\"  - {e}\" for e in errors)\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        # Ensure endpoint has trailing slash\n",
    "        if not self.endpoint.endswith(\"/\"):\n",
    "            self.endpoint += \"/\"\n",
    "        \n",
    "        logger.info(\"Azure OpenAI configuration validated successfully\")\n",
    "    \n",
    "    def get_model(\n",
    "        self,\n",
    "        deployment_name: Optional[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: Optional[int] = None,\n",
    "    ) -> AzureChatOpenAI:\n",
    "        \"\"\"Get configured Azure OpenAI model instance.\"\"\"\n",
    "        deployment = deployment_name or self.deployment_name\n",
    "        \n",
    "        return AzureChatOpenAI(\n",
    "            azure_deployment=deployment,\n",
    "            api_version=self.api_version,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            azure_endpoint=self.endpoint,\n",
    "            api_key=self.api_key,\n",
    "            timeout=self.request_timeout,\n",
    "        )\n",
    "    \n",
    "    def health_check(self) -> Dict[str, Any]:\n",
    "        \"\"\"Perform health check on Azure OpenAI connection.\"\"\"\n",
    "        try:\n",
    "            model = self.get_model()\n",
    "            response = model.invoke([HumanMessage(content=\"Health check\")])\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"healthy\",\n",
    "                \"endpoint\": self.endpoint,\n",
    "                \"deployment\": self.deployment_name,\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Health check failed: {e}\")\n",
    "            return {\n",
    "                \"status\": \"unhealthy\",\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProductionAgentState(AgentState):\n",
    "    \"\"\"Extended agent state for production use.\"\"\"\n",
    "    \n",
    "    # User context\n",
    "    user_id: NotRequired[str]\n",
    "    session_id: NotRequired[str]\n",
    "    user_metadata: NotRequired[Dict[str, Any]]\n",
    "    \n",
    "    # Tracking\n",
    "    model_call_count: NotRequired[int]\n",
    "    tool_call_count: NotRequired[int]\n",
    "    total_tokens: NotRequired[int]\n",
    "    conversation_start_time: NotRequired[float]\n",
    "    \n",
    "    # Rate limiting\n",
    "    rate_limit_remaining: NotRequired[int]\n",
    "    \n",
    "    # Safety\n",
    "    safety_violations: NotRequired[int]\n",
    "    content_flags: NotRequired[List[str]]\n",
    "    \n",
    "    # Performance\n",
    "    total_latency_ms: NotRequired[float]\n",
    "    last_model_latency_ms: NotRequired[float]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "533b8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProductionLoggingMiddleware(AgentMiddleware[ProductionAgentState]):\n",
    "    \"\"\"Production-grade logging with structured output and metrics.\"\"\"\n",
    "    \n",
    "    state_schema = ProductionAgentState\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.logger = logging.getLogger(\"ProductionLogging\")\n",
    "        self._start_time = None\n",
    "    \n",
    "    def before_agent(self, state: ProductionAgentState, runtime: Runtime) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize tracking at agent start.\"\"\"\n",
    "        self._start_time = time.time()\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Agent started\",\n",
    "            extra={\n",
    "                \"user_id\": state.get(\"user_id\", \"unknown\"),\n",
    "                \"session_id\": state.get(\"session_id\", \"unknown\"),\n",
    "                \"initial_messages\": len(state.get(\"messages\", [])),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"model_call_count\": 0,\n",
    "            \"tool_call_count\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"conversation_start_time\": self._start_time,\n",
    "            \"total_latency_ms\": 0,\n",
    "        }\n",
    "    \n",
    "    def before_model(self, state: ProductionAgentState, runtime: Runtime) -> Dict[str, Any]:\n",
    "        \"\"\"Log before model call.\"\"\"\n",
    "        count = state.get(\"model_call_count\", 0) + 1\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Model call #{count}\",\n",
    "            extra={\n",
    "                \"user_id\": state.get(\"user_id\"),\n",
    "                \"message_count\": len(state.get(\"messages\", [])),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\"model_call_count\": count}\n",
    "    \n",
    "    def after_model(self, state: ProductionAgentState, runtime: Runtime) -> Dict[str, Any]:\n",
    "        \"\"\"Track model response metrics.\"\"\"\n",
    "        updates = {}\n",
    "        \n",
    "        if state.get(\"messages\"):\n",
    "            last_msg = state[\"messages\"][-1]\n",
    "            \n",
    "            # Estimate tokens\n",
    "            if hasattr(last_msg, \"content\"):\n",
    "                tokens = len(str(last_msg.content)) // 4\n",
    "                updates[\"total_tokens\"] = state.get(\"total_tokens\", 0) + tokens\n",
    "            \n",
    "            # Count tool calls\n",
    "            if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
    "                tool_count = len(last_msg.tool_calls)\n",
    "                updates[\"tool_call_count\"] = state.get(\"tool_call_count\", 0) + tool_count\n",
    "                \n",
    "                self.logger.info(\n",
    "                    f\"Tools requested: {tool_count}\",\n",
    "                    extra={\n",
    "                        \"tools\": [tc.get(\"name\") for tc in last_msg.tool_calls]\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "        return updates\n",
    "    \n",
    "    def after_agent(self, state: ProductionAgentState, runtime: Runtime) -> Dict[str, Any]:\n",
    "        \"\"\"Log final metrics.\"\"\"\n",
    "        if self._start_time:\n",
    "            duration = time.time() - self._start_time\n",
    "            \n",
    "            self.logger.info(\n",
    "                \"Agent completed\",\n",
    "                extra={\n",
    "                    \"user_id\": state.get(\"user_id\"),\n",
    "                    \"duration_seconds\": round(duration, 2),\n",
    "                    \"model_calls\": state.get(\"model_call_count\", 0),\n",
    "                    \"tool_calls\": state.get(\"tool_call_count\", 0),\n",
    "                    \"total_messages\": len(state.get(\"messages\", [])),\n",
    "                    \"total_tokens\": state.get(\"total_tokens\", 0),\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "class ProductionRetryMiddleware(AgentMiddleware):\n",
    "    \"\"\"Production retry logic with exponential backoff and circuit breaking.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.max_retries = max_retries\n",
    "        self.base_delay = base_delay\n",
    "        self.logger = logging.getLogger(\"ProductionRetry\")\n",
    "        \n",
    "        # Circuit breaker state\n",
    "        self.failure_count = 0\n",
    "        self.circuit_open = False\n",
    "        self.circuit_open_until = 0\n",
    "    \n",
    "    def _is_circuit_open(self) -> bool:\n",
    "        \"\"\"Check if circuit breaker is open.\"\"\"\n",
    "        if self.circuit_open and time.time() > self.circuit_open_until:\n",
    "            self.circuit_open = False\n",
    "            self.failure_count = 0\n",
    "            self.logger.info(\"Circuit breaker closed - retrying operations\")\n",
    "        \n",
    "        return self.circuit_open\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler,\n",
    "    ) -> ModelResponse:\n",
    "        \"\"\"Wrap model calls with retry logic and circuit breaking.\"\"\"\n",
    "        \n",
    "        # Check circuit breaker\n",
    "        if self._is_circuit_open():\n",
    "            raise RuntimeError(\"Circuit breaker is open - too many failures\")\n",
    "        \n",
    "        last_exception = None\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = handler(request)\n",
    "                \n",
    "                # Reset failure count on success\n",
    "                self.failure_count = 0\n",
    "                \n",
    "                if attempt > 0:\n",
    "                    self.logger.info(f\"Request succeeded on retry {attempt + 1}\")\n",
    "                \n",
    "                return response\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_exception = e\n",
    "                self.failure_count += 1\n",
    "                \n",
    "                # Open circuit breaker if too many failures\n",
    "                if self.failure_count >= 5:\n",
    "                    self.circuit_open = True\n",
    "                    self.circuit_open_until = time.time() + 60  # Open for 1 minute\n",
    "                    self.logger.error(\"Circuit breaker opened due to repeated failures\")\n",
    "                    raise\n",
    "                \n",
    "                if attempt < self.max_retries - 1:\n",
    "                    delay = self.base_delay * (2 ** attempt)\n",
    "                    self.logger.warning(\n",
    "                        f\"Attempt {attempt + 1} failed: {str(e)}. Retrying in {delay}s...\"\n",
    "                    )\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    self.logger.error(f\"All {self.max_retries} attempts failed\")\n",
    "        \n",
    "        raise last_exception\n",
    "\n",
    "\n",
    "class ProductionRateLimitMiddleware(AgentMiddleware[ProductionAgentState]):\n",
    "    \"\"\"Production rate limiting with per-user quotas and monitoring.\"\"\"\n",
    "    \n",
    "    state_schema = ProductionAgentState\n",
    "    \n",
    "    def __init__(self, max_calls_per_minute: int = 60):\n",
    "        super().__init__()\n",
    "        self.max_calls_per_minute = max_calls_per_minute\n",
    "        self.logger = logging.getLogger(\"ProductionRateLimit\")\n",
    "        \n",
    "        # Per-user tracking\n",
    "        self.user_calls: Dict[str, List[float]] = {}\n",
    "    \n",
    "    def _check_rate_limit(self, user_id: str) -> bool:\n",
    "        \"\"\"Check if user is within rate limit.\"\"\"\n",
    "        now = time.time()\n",
    "        cutoff = now - 60  # 1 minute window\n",
    "        \n",
    "        # Initialize or clean old calls\n",
    "        if user_id not in self.user_calls:\n",
    "            self.user_calls[user_id] = []\n",
    "        \n",
    "        # Remove calls outside window\n",
    "        self.user_calls[user_id] = [\n",
    "            t for t in self.user_calls[user_id] if t > cutoff\n",
    "        ]\n",
    "        \n",
    "        # Check limit\n",
    "        if len(self.user_calls[user_id]) >= self.max_calls_per_minute:\n",
    "            return False\n",
    "        \n",
    "        # Record this call\n",
    "        self.user_calls[user_id].append(now)\n",
    "        return True\n",
    "    \n",
    "    @hook_config(can_jump_to=[\"end\"])\n",
    "    def before_model(\n",
    "        self,\n",
    "        state: ProductionAgentState,\n",
    "        runtime: Runtime\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Enforce rate limits before model call.\"\"\"\n",
    "        user_id = state.get(\"user_id\", \"default\")\n",
    "        \n",
    "        if not self._check_rate_limit(user_id):\n",
    "            remaining_calls = len([\n",
    "                t for t in self.user_calls[user_id]\n",
    "                if t > time.time() - 60\n",
    "            ])\n",
    "            \n",
    "            self.logger.warning(\n",
    "                f\"Rate limit exceeded for user {user_id}\",\n",
    "                extra={\"calls_in_window\": remaining_calls}\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    AIMessage(\n",
    "                        content=(\n",
    "                            f\"Rate limit exceeded. You can make \"\n",
    "                            f\"{self.max_calls_per_minute} requests per minute. \"\n",
    "                            f\"Please wait before trying again.\"\n",
    "                        )\n",
    "                    )\n",
    "                ],\n",
    "                \"jump_to\": \"end\"\n",
    "            }\n",
    "        \n",
    "        # Update remaining count\n",
    "        remaining = self.max_calls_per_minute - len(self.user_calls[user_id])\n",
    "        return {\"rate_limit_remaining\": remaining}\n",
    "\n",
    "\n",
    "class ProductionSafetyMiddleware(AgentMiddleware[ProductionAgentState]):\n",
    "    \"\"\"Production safety and content moderation.\"\"\"\n",
    "    \n",
    "    state_schema = ProductionAgentState\n",
    "    \n",
    "    def __init__(self, max_messages: int = 100, max_violations: int = 3):\n",
    "        super().__init__()\n",
    "        self.max_messages = max_messages\n",
    "        self.max_violations = max_violations\n",
    "        self.logger = logging.getLogger(\"ProductionSafety\")\n",
    "        \n",
    "        # Content filters\n",
    "        self.blocked_patterns = [\n",
    "            \"ignore previous instructions\",\n",
    "            \"disregard your guidelines\",\n",
    "            \"bypass safety\",\n",
    "            \"jailbreak\",\n",
    "            \"prompt injection\",\n",
    "        ]\n",
    "    \n",
    "    @hook_config(can_jump_to=[\"end\"])\n",
    "    def before_model(\n",
    "        self,\n",
    "        state: ProductionAgentState,\n",
    "        runtime: Runtime\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Perform safety checks before model call.\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        violations = state.get(\"safety_violations\", 0)\n",
    "        \n",
    "        # Check conversation length\n",
    "        if len(messages) >= self.max_messages:\n",
    "            self.logger.warning(\n",
    "                f\"Conversation length limit reached: {len(messages)} messages\"\n",
    "            )\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    AIMessage(\n",
    "                        content=f\"Conversation limit of {self.max_messages} messages reached. \"\n",
    "                                \"Please start a new conversation.\"\n",
    "                    )\n",
    "                ],\n",
    "                \"jump_to\": \"end\"\n",
    "            }\n",
    "        \n",
    "        # Check violation count\n",
    "        if violations >= self.max_violations:\n",
    "            self.logger.error(f\"Maximum violations reached: {violations}\")\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    AIMessage(\n",
    "                        content=\"This conversation has been terminated due to safety policy violations.\"\n",
    "                    )\n",
    "                ],\n",
    "                \"jump_to\": \"end\"\n",
    "            }\n",
    "        \n",
    "        # Content filtering\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            if hasattr(last_message, \"content\"):\n",
    "                content = str(last_message.content).lower()\n",
    "                \n",
    "                for pattern in self.blocked_patterns:\n",
    "                    if pattern.lower() in content:\n",
    "                        new_violations = violations + 1\n",
    "                        \n",
    "                        self.logger.warning(\n",
    "                            f\"Blocked pattern detected: '{pattern}'\",\n",
    "                            extra={\"user_id\": state.get(\"user_id\")}\n",
    "                        )\n",
    "                        \n",
    "                        return {\n",
    "                            \"messages\": [\n",
    "                                AIMessage(\n",
    "                                    content=f\"Content policy violation detected. \"\n",
    "                                            f\"({new_violations}/{self.max_violations} strikes)\"\n",
    "                                )\n",
    "                            ],\n",
    "                            \"safety_violations\": new_violations,\n",
    "                            \"content_flags\": state.get(\"content_flags\", []) + [pattern],\n",
    "                            \"jump_to\": \"end\"\n",
    "                        }\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "class AzureDynamicModelMiddleware(AgentMiddleware[ProductionAgentState]):\n",
    "    \"\"\"Production-grade dynamic Azure deployment selection.\"\"\"\n",
    "    \n",
    "    state_schema = ProductionAgentState\n",
    "    \n",
    "    def __init__(self, config: AzureOpenAIConfig, complexity_threshold: int = 10):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.complexity_threshold = complexity_threshold\n",
    "        self.logger = logging.getLogger(\"AzureModelSelection\")\n",
    "        \n",
    "        # Initialize models\n",
    "        self.simple_model = config.get_model(config.deployment_name)\n",
    "        self.complex_model = config.get_model(config.gpt4_deployment_name)\n",
    "    \n",
    "    def _analyze_complexity(self, request: ModelRequest) -> str:\n",
    "        \"\"\"Analyze request complexity.\"\"\"\n",
    "        message_count = len(request.messages)\n",
    "        \n",
    "        # Long conversations need better model\n",
    "        if message_count > self.complexity_threshold:\n",
    "            return \"complex\"\n",
    "        \n",
    "        # Check for complexity indicators\n",
    "        if request.messages:\n",
    "            last_msg = request.messages[-1]\n",
    "            if hasattr(last_msg, \"content\"):\n",
    "                content = str(last_msg.content).lower()\n",
    "                \n",
    "                # Complexity keywords\n",
    "                if any(kw in content for kw in [\n",
    "                    \"analyze\", \"detailed\", \"comprehensive\", \"technical\",\n",
    "                    \"explain in detail\", \"step by step\"\n",
    "                ]):\n",
    "                    return \"complex\"\n",
    "                \n",
    "                # Long messages\n",
    "                if len(content) > 500:\n",
    "                    return \"complex\"\n",
    "        \n",
    "        return \"simple\"\n",
    "    \n",
    "    def wrap_model_call(self, request: ModelRequest, handler) -> ModelResponse:\n",
    "        \"\"\"Select appropriate Azure deployment.\"\"\"\n",
    "        complexity = self._analyze_complexity(request)\n",
    "        \n",
    "        if complexity == \"complex\":\n",
    "            model = self.complex_model\n",
    "            deployment = self.config.gpt4_deployment_name\n",
    "        else:\n",
    "            model = self.simple_model\n",
    "            deployment = self.config.deployment_name\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Selected deployment: {deployment} (complexity: {complexity})\"\n",
    "        )\n",
    "        \n",
    "        return handler(request.override(model=model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "899f7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_production_tools() -> List[Tool]:\n",
    "    \"\"\"Create production-ready tools with error handling.\"\"\"\n",
    "    \n",
    "    # Search tool with error handling\n",
    "    def safe_search(query: str) -> str:\n",
    "        \"\"\"Perform web search with error handling.\"\"\"\n",
    "        try:\n",
    "            search = DuckDuckGoSearchRun()\n",
    "            result = search.run(query)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Search failed: {e}\")\n",
    "            return f\"Search temporarily unavailable. Error: {str(e)}\"\n",
    "    \n",
    "    search_tool = Tool(\n",
    "        name=\"web_search\",\n",
    "        func=safe_search,\n",
    "        description=(\n",
    "            \"Search the web for current information. \"\n",
    "            \"Use this when you need up-to-date facts or recent events.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Calculator tool\n",
    "    def safe_calculate(expression: str) -> str:\n",
    "        \"\"\"Safely evaluate mathematical expressions.\"\"\"\n",
    "        try:\n",
    "            # Security: only allow safe characters\n",
    "            allowed = set(\"0123456789+-*/.() \")\n",
    "            if not all(c in allowed for c in expression):\n",
    "                return \"Error: Invalid characters in expression\"\n",
    "            \n",
    "            result = eval(expression)\n",
    "            return f\"Result: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Calculation error: {str(e)}\"\n",
    "    \n",
    "    calc_tool = Tool(\n",
    "        name=\"calculator\",\n",
    "        func=safe_calculate,\n",
    "        description=(\n",
    "            \"Perform mathematical calculations. \"\n",
    "            \"Input should be a valid mathematical expression like '2 + 2' or '(15 * 8) / 4'.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return [search_tool, calc_tool]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "188ed0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_production_tools() -> List[Tool]:\n",
    "    \"\"\"Create production-ready tools with error handling.\"\"\"\n",
    "    \n",
    "    # Search tool with error handling\n",
    "    def safe_search(query: str) -> str:\n",
    "        \"\"\"Perform web search with error handling.\"\"\"\n",
    "        try:\n",
    "            search = DuckDuckGoSearchRun()\n",
    "            result = search.run(query)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Search failed: {e}\")\n",
    "            return f\"Search temporarily unavailable. Error: {str(e)}\"\n",
    "    \n",
    "    search_tool = Tool(\n",
    "        name=\"web_search\",\n",
    "        func=safe_search,\n",
    "        description=(\n",
    "            \"Search the web for current information. \"\n",
    "            \"Use this when you need up-to-date facts or recent events.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Calculator tool\n",
    "    def safe_calculate(expression: str) -> str:\n",
    "        \"\"\"Safely evaluate mathematical expressions.\"\"\"\n",
    "        try:\n",
    "            # Security: only allow safe characters\n",
    "            allowed = set(\"0123456789+-*/.() \")\n",
    "            if not all(c in allowed for c in expression):\n",
    "                return \"Error: Invalid characters in expression\"\n",
    "            \n",
    "            result = eval(expression)\n",
    "            return f\"Result: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Calculation error: {str(e)}\"\n",
    "    \n",
    "    calc_tool = Tool(\n",
    "        name=\"calculator\",\n",
    "        func=safe_calculate,\n",
    "        description=(\n",
    "            \"Perform mathematical calculations. \"\n",
    "            \"Input should be a valid mathematical expression like '2 + 2' or '(15 * 8) / 4'.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return [search_tool, calc_tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d506ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProductionAgent:\n",
    "    \"\"\"Production-ready Azure OpenAI agent with full middleware stack.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize production agent.\"\"\"\n",
    "        logger.info(\"Initializing production agent...\")\n",
    "        \n",
    "        # Load configuration\n",
    "        self.config = AzureOpenAIConfig()\n",
    "        \n",
    "        # Perform health check\n",
    "        health = self.config.health_check()\n",
    "        if health[\"status\"] != \"healthy\":\n",
    "            raise RuntimeError(f\"Azure OpenAI health check failed: {health}\")\n",
    "        \n",
    "        logger.info(\"Azure OpenAI health check passed\")\n",
    "        \n",
    "        # Initialize middleware\n",
    "        self.middleware = self._create_middleware_stack()\n",
    "        \n",
    "        # Create tools\n",
    "        self.tools = create_production_tools()\n",
    "        \n",
    "        # Create agent\n",
    "        self.agent = self._create_agent()\n",
    "        \n",
    "        logger.info(\"Production agent initialized successfully\")\n",
    "    \n",
    "    def _create_middleware_stack(self) -> List[AgentMiddleware]:\n",
    "        \"\"\"Create production middleware stack.\"\"\"\n",
    "        return [\n",
    "            ProductionSafetyMiddleware(\n",
    "                max_messages=self.config.max_conversation_messages,\n",
    "                max_violations=3\n",
    "            ),\n",
    "            ProductionRateLimitMiddleware(\n",
    "                max_calls_per_minute=self.config.rate_limit_calls\n",
    "            ),\n",
    "            AzureDynamicModelMiddleware(\n",
    "                config=self.config,\n",
    "                complexity_threshold=10\n",
    "            ),\n",
    "            ProductionRetryMiddleware(\n",
    "                max_retries=self.config.max_retries,\n",
    "                base_delay=1.0\n",
    "            ),\n",
    "            ProductionLoggingMiddleware(),\n",
    "        ]\n",
    "    \n",
    "    def _create_agent(self):\n",
    "        \"\"\"Create LangChain agent with middleware.\"\"\"\n",
    "        default_model = self.config.get_model()\n",
    "        \n",
    "        system_prompt = (\n",
    "            \"You are a helpful AI assistant powered by Azure OpenAI. \"\n",
    "            \"You have access to web search and calculation tools. \"\n",
    "            \"Provide accurate, helpful, and concise responses. \"\n",
    "            \"Use tools when appropriate to give the best answers.\"\n",
    "        )\n",
    "        \n",
    "        return create_agent(\n",
    "            model=default_model,\n",
    "            middleware=self.middleware,\n",
    "            tools=self.tools,\n",
    "            system_prompt=system_prompt,\n",
    "        )\n",
    "    \n",
    "    def invoke(\n",
    "        self,\n",
    "        message: str,\n",
    "        user_id: str = \"default\",\n",
    "        session_id: Optional[str] = None,\n",
    "        conversation_history: Optional[List] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Invoke agent with a message.\n",
    "        \n",
    "        Args:\n",
    "            message: User message\n",
    "            user_id: User identifier\n",
    "            session_id: Session identifier\n",
    "            conversation_history: Previous messages\n",
    "        \n",
    "        Returns:\n",
    "            Response dictionary with message and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Build message list\n",
    "            messages = conversation_history or []\n",
    "            messages.append(HumanMessage(content=message))\n",
    "            \n",
    "            # Create state\n",
    "            state = {\n",
    "                \"messages\": messages,\n",
    "                \"user_id\": user_id,\n",
    "                \"session_id\": session_id or f\"session_{int(time.time())}\",\n",
    "                \"safety_violations\": 0,\n",
    "            }\n",
    "            \n",
    "            # Invoke agent\n",
    "            logger.info(f\"Processing message for user {user_id}\")\n",
    "            result = self.agent.invoke(state)\n",
    "            \n",
    "            # Extract response\n",
    "            if result.get(\"messages\"):\n",
    "                response_message = result[\"messages\"][-1]\n",
    "                response_content = response_message.content if hasattr(response_message, \"content\") else str(response_message)\n",
    "            else:\n",
    "                response_content = \"No response generated\"\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"response\": response_content,\n",
    "                \"metadata\": {\n",
    "                    \"user_id\": user_id,\n",
    "                    \"session_id\": result.get(\"session_id\"),\n",
    "                    \"model_calls\": result.get(\"model_call_count\", 0),\n",
    "                    \"tool_calls\": result.get(\"tool_call_count\", 0),\n",
    "                    \"tokens\": result.get(\"total_tokens\", 0),\n",
    "                    \"rate_limit_remaining\": result.get(\"rate_limit_remaining\", 0),\n",
    "                },\n",
    "                \"conversation_state\": result,\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Agent invocation failed: {e}\", exc_info=True)\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"response\": \"I apologize, but I encountered an error processing your request.\",\n",
    "            }\n",
    "    \n",
    "    def health_check(self) -> Dict[str, Any]:\n",
    "        \"\"\"Perform health check on agent and dependencies.\"\"\"\n",
    "        return {\n",
    "            \"agent\": \"healthy\",\n",
    "            \"azure_openai\": self.config.health_check(),\n",
    "            \"middleware_count\": len(self.middleware),\n",
    "            \"tools_count\": len(self.tools),\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main application entry point.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PRODUCTION AZURE OPENAI AGENT\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Initialize agent\n",
    "        print(\"Initializing production agent...\")\n",
    "        agent = ProductionAgent()\n",
    "        print(\"✓ Agent initialized successfully\\n\")\n",
    "        \n",
    "        # Perform health check\n",
    "        print(\"Performing health check...\")\n",
    "        health = agent.health_check()\n",
    "        print(f\"✓ Health check passed: {json.dumps(health, indent=2)}\\n\")\n",
    "        \n",
    "        # Interactive loop\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Agent is ready. Type 'quit' to exit, 'health' for health check\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "        \n",
    "        conversation_history = []\n",
    "        user_id = \"demo_user\"\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nYou: \").strip()\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                if user_input.lower() == 'quit':\n",
    "                    print(\"\\nGoodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if user_input.lower() == 'health':\n",
    "                    health = agent.health_check()\n",
    "                    print(f\"\\nHealth Status:\\n{json.dumps(health, indent=2)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Invoke agent\n",
    "                result = agent.invoke(\n",
    "                    message=user_input,\n",
    "                    user_id=user_id,\n",
    "                    conversation_history=conversation_history,\n",
    "                )\n",
    "                \n",
    "                # Display response\n",
    "                if result[\"success\"]:\n",
    "                    print(f\"\\nAssistant: {result['response']}\")\n",
    "                    \n",
    "                    # Show metadata\n",
    "                    metadata = result[\"metadata\"]\n",
    "                    print(f\"\\n[Stats: Calls={metadata['model_calls']}, \"\n",
    "                          f\"Tools={metadata['tool_calls']}, \"\n",
    "                          f\"Tokens≈{metadata['tokens']}, \"\n",
    "                          f\"Rate limit remaining={metadata['rate_limit_remaining']}]\")\n",
    "                    \n",
    "                    # Update conversation history\n",
    "                    conversation_history = result[\"conversation_state\"][\"messages\"]\n",
    "                else:\n",
    "                    print(f\"\\nError: {result['error']}\")\n",
    "                    print(f\"Assistant: {result['response']}\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nInterrupted. Goodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in main loop: {e}\", exc_info=True)\n",
    "                print(f\"\\nError: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error: {e}\", exc_info=True)\n",
    "        print(f\"\\n Fatal error: {e}\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
