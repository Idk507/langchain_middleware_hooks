{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "145ffcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from typing import Dict, Any, List, Optional\n",
    "from typing_extensions import TypedDict, NotRequired\n",
    "import requests\n",
    "from langchain.agents.middleware import AgentState, AgentMiddleware,ModelRequest,ModelResponse,hook_config\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os \n",
    "from dotenv import load_dotenv \n",
    "from langchain_core.tools import Tool\n",
    "import logging\n",
    "import sys\n",
    "from langgraph.runtime import Runtime\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6af70c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AzureOpenAIConfig:\n",
    "    \"\"\"Production Azure OpenAI configuration with validation and health checks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize and validate Azure OpenAI configuration.\"\"\"\n",
    "        # Load configuration from environment\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        self.endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        self.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "        self.deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4.1\")\n",
    "        self.gpt4_deployment_name = os.getenv(\"AZURE_OPENAI_GPT4_DEPLOYMENT_NAME\", \"gpt-4.1\")\n",
    "        \n",
    "        # Configuration limits\n",
    "        self.max_retries = int(os.getenv(\"MAX_RETRIES\", \"3\"))\n",
    "        self.rate_limit_calls = int(os.getenv(\"RATE_LIMIT_CALLS_PER_MINUTE\", \"60\"))\n",
    "        self.max_conversation_messages = int(os.getenv(\"MAX_CONVERSATION_MESSAGES\", \"100\"))\n",
    "        self.request_timeout = int(os.getenv(\"REQUEST_TIMEOUT\", \"60\"))\n",
    "        \n",
    "        # Validate configuration\n",
    "        self._validate()\n",
    "        \n",
    "    def _validate(self):\n",
    "        \"\"\"Validate configuration and fail fast if invalid.\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        if not self.api_key:\n",
    "            errors.append(\"AZURE_OPENAI_API_KEY is required\")\n",
    "        \n",
    "        if not self.endpoint:\n",
    "            errors.append(\"AZURE_OPENAI_ENDPOINT is required\")\n",
    "        elif not self.endpoint.startswith(\"https://\"):\n",
    "            errors.append(\"AZURE_OPENAI_ENDPOINT must start with https://\")\n",
    "        \n",
    "        if not self.deployment_name:\n",
    "            errors.append(\"AZURE_OPENAI_DEPLOYMENT_NAME is required\")\n",
    "        \n",
    "        if errors:\n",
    "            error_msg = \"Configuration validation failed:\\n\" + \"\\n\".join(f\"  - {e}\" for e in errors)\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        # Ensure endpoint has trailing slash\n",
    "        if not self.endpoint.endswith(\"/\"):\n",
    "            self.endpoint += \"/\"\n",
    "        \n",
    "        logger.info(\"Azure OpenAI configuration validated successfully\")\n",
    "    \n",
    "    def get_model(\n",
    "        self,\n",
    "        deployment_name: Optional[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: Optional[int] = None,\n",
    "    ) -> AzureChatOpenAI:\n",
    "        \"\"\"Get configured Azure OpenAI model instance.\"\"\"\n",
    "        deployment = deployment_name or self.deployment_name\n",
    "        \n",
    "        return AzureChatOpenAI(\n",
    "            azure_deployment=deployment,\n",
    "            api_version=self.api_version,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            azure_endpoint=self.endpoint,\n",
    "            api_key=self.api_key,\n",
    "            timeout=self.request_timeout,\n",
    "        )\n",
    "    \n",
    "    def health_check(self) -> Dict[str, Any]:\n",
    "        \"\"\"Perform health check on Azure OpenAI connection.\"\"\"\n",
    "        try:\n",
    "            model = self.get_model()\n",
    "            response = model.invoke([HumanMessage(content=\"Health check\")])\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"healthy\",\n",
    "                \"endpoint\": self.endpoint,\n",
    "                \"deployment\": self.deployment_name,\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Health check failed: {e}\")\n",
    "            return {\n",
    "                \"status\": \"unhealthy\",\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8aefbcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('agent.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20092dfa",
   "metadata": {},
   "source": [
    "# CUSTOM STATE SCHEMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "142e1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM STATE SCHEMA\n",
    "# ============================================================================\n",
    "\n",
    "class ProductionAgentState(AgentState):\n",
    "    \"\"\"Extended agent state for production use.\"\"\"\n",
    "    \n",
    "    # User context\n",
    "    user_id: NotRequired[str]\n",
    "    session_id: NotRequired[str]\n",
    "    user_metadata: NotRequired[Dict[str, Any]]\n",
    "    \n",
    "    # Tracking\n",
    "    model_call_count: NotRequired[int]\n",
    "    tool_call_count: NotRequired[int]\n",
    "    total_tokens: NotRequired[int]\n",
    "    conversation_start_time: NotRequired[float]\n",
    "    \n",
    "    # Rate limiting\n",
    "    rate_limit_remaining: NotRequired[int]\n",
    "    \n",
    "    # Safety\n",
    "    safety_violations: NotRequired[int]\n",
    "    content_flags: NotRequired[List[str]]\n",
    "    \n",
    "    # Performance\n",
    "    total_latency_ms: NotRequired[float]\n",
    "    last_model_latency_ms: NotRequired[float]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a0f59",
   "metadata": {},
   "source": [
    "\n",
    "# PRODUCTION MIDDLEWARE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27d2e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProductionLoggingMiddleware(AgentMiddleware[ProductionAgentState]):\n",
    "    \"\"\"Production-grade logging with structured output and metrics.\"\"\"\n",
    "    \n",
    "    state_schema = ProductionAgentState\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.logger = logging.getLogger(\"ProductionLogging\")\n",
    "        self._start_time = None\n",
    "    \n",
    "    def before_agent(self, state: ProductionAgentState, runtime: Runtime) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize tracking at agent start.\"\"\"\n",
    "        self._start_time = time.time()\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Agent started\",\n",
    "            extra={\n",
    "                \"user_id\": state.get(\"user_id\", \"unknown\"),\n",
    "                \"session_id\": state.get(\"session_id\", \"unknown\"),\n",
    "                \"initial_messages\": len(state.get(\"messages\", [])),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"model_call_count\": 0,\n",
    "            \"tool_call_count\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"conversation_start_time\": self._start_time,\n",
    "            \"total_latency_ms\": 0,\n",
    "        }\n",
    "    \n",
    "    def before_model(self, state: ProductionAgentState, runtime: Runtime) -> Dict[str, Any]:\n",
    "        \"\"\"Log before model call.\"\"\"\n",
    "        count = state.get(\"model_call_count\", 0) + 1\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Model call #{count}\",\n",
    "            extra={\n",
    "                \"user_id\": state.get(\"user_id\"),\n",
    "                \"message_count\": len(state.get(\"messages\", [])),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\"model_call_count\": count}\n",
    "    \n",
    "    def after_model(self, state: ProductionAgentState, runtime: Runtime) -> Dict[str, Any]:\n",
    "        \"\"\"Track model response metrics.\"\"\"\n",
    "        updates = {}\n",
    "        \n",
    "        if state.get(\"messages\"):\n",
    "            last_msg = state[\"messages\"][-1]\n",
    "            \n",
    "            # Estimate tokens\n",
    "            if hasattr(last_msg, \"content\"):\n",
    "                tokens = len(str(last_msg.content)) // 4\n",
    "                updates[\"total_tokens\"] = state.get(\"total_tokens\", 0) + tokens\n",
    "            \n",
    "            # Count tool calls\n",
    "            if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
    "                tool_count = len(last_msg.tool_calls)\n",
    "                updates[\"tool_call_count\"] = state.get(\"tool_call_count\", 0) + tool_count\n",
    "                \n",
    "                self.logger.info(\n",
    "                    f\"Tools requested: {tool_count}\",\n",
    "                    extra={\n",
    "                        \"tools\": [tc.get(\"name\") for tc in last_msg.tool_calls]\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "        return updates\n",
    "    \n",
    "    def after_agent(self, state: ProductionAgentState, runtime: Runtime) -> Dict[str, Any]:\n",
    "        \"\"\"Log final metrics.\"\"\"\n",
    "        if self._start_time:\n",
    "            duration = time.time() - self._start_time\n",
    "            \n",
    "            self.logger.info(\n",
    "                \"Agent completed\",\n",
    "                extra={\n",
    "                    \"user_id\": state.get(\"user_id\"),\n",
    "                    \"duration_seconds\": round(duration, 2),\n",
    "                    \"model_calls\": state.get(\"model_call_count\", 0),\n",
    "                    \"tool_calls\": state.get(\"tool_call_count\", 0),\n",
    "                    \"total_messages\": len(state.get(\"messages\", [])),\n",
    "                    \"total_tokens\": state.get(\"total_tokens\", 0),\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3321ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionRetryMiddleware(AgentMiddleware):\n",
    "    \"\"\" Production retry logic with exponential backoff and circuit breaker. \"\"\"\n",
    "    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.max_retries = max_retries\n",
    "        self.base_delay = base_delay\n",
    "        self.logger = logging.getLogger(\"ProductionRetry\")\n",
    "\n",
    "        # circuit breaker state\n",
    "        self.failure_count = 0\n",
    "        self.circuit_open = False\n",
    "        self.circuit_open_until = 0\n",
    "\n",
    "    def _is_circuit_open(self) -> bool:\n",
    "        if self.circuit_open and time.time() >= self.circuit_open_until:\n",
    "            self.logger.info(\"Circuit breaker reset\")\n",
    "            self.circuit_open = False\n",
    "            self.failure_count = 0\n",
    "        return self.circuit_open\n",
    "\n",
    "    def wrap_model_call(self, request : ModelRequest,\n",
    "                        handler ,) -> ModelResponse:\n",
    "        \"\"\" Wrap model call with retry logic. \"\"\"\n",
    "        #check circuit breaker\n",
    "        if self._is_circuit_open():\n",
    "            self.logger.warning(\"Circuit breaker open - rejecting model call\")\n",
    "            raise Exception(\"Service temporarily unavailable due to repeated failures\")\n",
    "\n",
    "        last_exception = None\n",
    "        for attempt in range(1, self.max_retries + 1):\n",
    "            try : \n",
    "                response = handler(request)\n",
    "                #reset failure count on success\n",
    "                self.failure_count = 0\n",
    "                if attempt > 1:\n",
    "                    self.logger.info(f\"Model call succeeded on attempt {attempt}\")\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                last_exception = e\n",
    "                self.failure_count += 1\n",
    "                self.logger.error(\n",
    "                    f\"Model call failed on attempt {attempt}: {str(e)}\",\n",
    "                    exc_info=True\n",
    "                )\n",
    "                if self.failure_count >= self.max_retries:\n",
    "                    self.circuit_open = True\n",
    "                    self.circuit_open_until = time.time() + 60  # open for 60 seconds\n",
    "                    self.logger.error(\"Circuit breaker opened due to repeated failures\")\n",
    "                    raise Exception(\"Service temporarily unavailable due to repeated failures\") from e\n",
    "                else:\n",
    "                    delay = self.base_delay * (2 ** (attempt - 1))\n",
    "                    self.logger.info(f\"Retrying after {delay:.2f} seconds...\")\n",
    "                    time.sleep(delay)\n",
    "        # If we exhaust retries, raise the last exception\n",
    "        raise last_exception\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6cf85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionRateLimitMiddleware(AgentMiddleware[ProductionAgentState]):\n",
    "    \"\"\" Production rate limiting middleware that tracks API usage and enforces limits. \"\"\"\n",
    "    state_schema = ProductionAgentState\n",
    "\n",
    "    def __init__(self, max_calls_per_minute: int = 60):\n",
    "        super().__init__()\n",
    "        self.max_calls_per_minute = max_calls_per_minute\n",
    "        self.logger = logging.getLogger(\"ProductionRateLimit\")\n",
    "\n",
    "        # per user tracking\n",
    "        self.user_calls : Dict[str, List[float]] = {}\n",
    "\n",
    "    def _check_rate_limit(self, user_id : str) -> bool:\n",
    "        \"\"\"Check if user is within rate limit.\"\"\"\n",
    "        now = time.time()\n",
    "        cutoff = now - 60  # 1 minute window\n",
    "\n",
    "        # initialize tracking for new user\n",
    "        if user_id not in self.user_calls:\n",
    "            self.user_calls[user_id] = []\n",
    "        \n",
    "        # remove old calls\n",
    "        self.user_calls[user_id] = [\n",
    "            call_time for call_time in self.user_calls[user_id]\n",
    "            if call_time > cutoff\n",
    "        ]\n",
    "\n",
    "        # check limits \n",
    "        if len(self.user_calls[user_id]) >= self.max_calls_per_minute:\n",
    "            self.logger.warning(f\"User {user_id} exceeded rate limit\")\n",
    "            return False\n",
    "\n",
    "        # record this call\n",
    "        self.user_calls[user_id].append(now)\n",
    "        return True\n",
    "\n",
    "    @hook_config(can_jump_to=[\"end\"])\n",
    "    def before_model(\n",
    "        self,\n",
    "        state: ProductionAgentState,\n",
    "        runtime: Runtime\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Enforce rate limits before model call.\"\"\"\n",
    "        user_id = state.get(\"user_id\", \"default\")\n",
    "        \n",
    "        if not self._check_rate_limit(user_id):\n",
    "            remaining_calls = len([\n",
    "                t for t in self.user_calls[user_id]\n",
    "                if t > time.time() - 60\n",
    "            ])\n",
    "            \n",
    "            self.logger.warning(\n",
    "                f\"Rate limit exceeded for user {user_id}\",\n",
    "                extra={\"calls_in_window\": remaining_calls}\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    AIMessage(\n",
    "                        content=(\n",
    "                            f\"Rate limit exceeded. You can make \"\n",
    "                            f\"{self.max_calls_per_minute} requests per minute. \"\n",
    "                            f\"Please wait before trying again.\"\n",
    "                        )\n",
    "                    )\n",
    "                ],\n",
    "                \"jump_to\": \"end\"\n",
    "            }\n",
    "        \n",
    "        # Update remaining count\n",
    "        remaining = self.max_calls_per_minute - len(self.user_calls[user_id])\n",
    "        return {\"rate_limit_remaining\": remaining}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9886294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProductionSafetyMiddleware(AgentMiddleware[ProductionAgentState]):\n",
    "    \"\"\"Production safety and content moderation.\"\"\"\n",
    "    \n",
    "    state_schema = ProductionAgentState\n",
    "    \n",
    "    def __init__(self, max_messages: int = 100, max_violations: int = 3):\n",
    "        super().__init__()\n",
    "        self.max_messages = max_messages\n",
    "        self.max_violations = max_violations\n",
    "        self.logger = logging.getLogger(\"ProductionSafety\")\n",
    "        \n",
    "        # Content filters\n",
    "        self.blocked_patterns = [\n",
    "            \"ignore previous instructions\",\n",
    "            \"disregard your guidelines\",\n",
    "            \"bypass safety\",\n",
    "            \"jailbreak\",\n",
    "            \"prompt injection\",\n",
    "        ]\n",
    "    \n",
    "    @hook_config(can_jump_to=[\"end\"])\n",
    "    def before_model(\n",
    "        self,\n",
    "        state: ProductionAgentState,\n",
    "        runtime: Runtime\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Perform safety checks before model call.\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        violations = state.get(\"safety_violations\", 0)\n",
    "        \n",
    "        # Check conversation length\n",
    "        if len(messages) >= self.max_messages:\n",
    "            self.logger.warning(\n",
    "                f\"Conversation length limit reached: {len(messages)} messages\"\n",
    "            )\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    AIMessage(\n",
    "                        content=f\"Conversation limit of {self.max_messages} messages reached. \"\n",
    "                                \"Please start a new conversation.\"\n",
    "                    )\n",
    "                ],\n",
    "                \"jump_to\": \"end\"\n",
    "            }\n",
    "        \n",
    "        # Check violation count\n",
    "        if violations >= self.max_violations:\n",
    "            self.logger.error(f\"Maximum violations reached: {violations}\")\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    AIMessage(\n",
    "                        content=\"This conversation has been terminated due to safety policy violations.\"\n",
    "                    )\n",
    "                ],\n",
    "                \"jump_to\": \"end\"\n",
    "            }\n",
    "        \n",
    "        # Content filtering\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            if hasattr(last_message, \"content\"):\n",
    "                content = str(last_message.content).lower()\n",
    "                \n",
    "                for pattern in self.blocked_patterns:\n",
    "                    if pattern.lower() in content:\n",
    "                        new_violations = violations + 1\n",
    "                        \n",
    "                        self.logger.warning(\n",
    "                            f\"Blocked pattern detected: '{pattern}'\",\n",
    "                            extra={\"user_id\": state.get(\"user_id\")}\n",
    "                        )\n",
    "                        \n",
    "                        return {\n",
    "                            \"messages\": [\n",
    "                                AIMessage(\n",
    "                                    content=f\"Content policy violation detected. \"\n",
    "                                            f\"({new_violations}/{self.max_violations} strikes)\"\n",
    "                                )\n",
    "                            ],\n",
    "                            \"safety_violations\": new_violations,\n",
    "                            \"content_flags\": state.get(\"content_flags\", []) + [pattern],\n",
    "                            \"jump_to\": \"end\"\n",
    "                        }\n",
    "        \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa3538b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AzureDynamicModelMiddleware(AgentMiddleware[ProductionAgentState]):\n",
    "    \"\"\"Production-grade dynamic Azure deployment selection.\"\"\"\n",
    "    \n",
    "    state_schema = ProductionAgentState\n",
    "    \n",
    "    def __init__(self, config: AzureOpenAIConfig, complexity_threshold: int = 10):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.complexity_threshold = complexity_threshold\n",
    "        self.logger = logging.getLogger(\"AzureModelSelection\")\n",
    "        \n",
    "        # Initialize models\n",
    "        self.simple_model = config.get_model(config.deployment_name)\n",
    "        self.complex_model = config.get_model(config.gpt4_deployment_name)\n",
    "    \n",
    "    def _analyze_complexity(self, request: ModelRequest) -> str:\n",
    "        \"\"\"Analyze request complexity.\"\"\"\n",
    "        message_count = len(request.messages)\n",
    "        \n",
    "        # Long conversations need better model\n",
    "        if message_count > self.complexity_threshold:\n",
    "            return \"complex\"\n",
    "        \n",
    "        # Check for complexity indicators\n",
    "        if request.messages:\n",
    "            last_msg = request.messages[-1]\n",
    "            if hasattr(last_msg, \"content\"):\n",
    "                content = str(last_msg.content).lower()\n",
    "                \n",
    "                # Complexity keywords\n",
    "                if any(kw in content for kw in [\n",
    "                    \"analyze\", \"detailed\", \"comprehensive\", \"technical\",\n",
    "                    \"explain in detail\", \"step by step\"\n",
    "                ]):\n",
    "                    return \"complex\"\n",
    "                \n",
    "                # Long messages\n",
    "                if len(content) > 500:\n",
    "                    return \"complex\"\n",
    "        \n",
    "        return \"simple\"\n",
    "    \n",
    "    def wrap_model_call(self, request: ModelRequest, handler) -> ModelResponse:\n",
    "        \"\"\"Select appropriate Azure deployment.\"\"\"\n",
    "        complexity = self._analyze_complexity(request)\n",
    "        \n",
    "        if complexity == \"complex\":\n",
    "            model = self.complex_model\n",
    "            deployment = self.config.gpt4_deployment_name\n",
    "        else:\n",
    "            model = self.simple_model\n",
    "            deployment = self.config.deployment_name\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Selected deployment: {deployment} (complexity: {complexity})\"\n",
    "        )\n",
    "        \n",
    "        return handler(request.override(model=model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3196d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
